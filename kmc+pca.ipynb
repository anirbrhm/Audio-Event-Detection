{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import librosa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('X_train.npy')\n",
    "y_train = np.load('y_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making data 0 mean and computing covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.mean(X_train, axis=0)\n",
    "X_train_subset = X_train - mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[549.04376947, 513.55137845, 479.7201979 , ..., 532.60787173,\n",
       "        532.36929241, 512.93402293],\n",
       "       [513.55137845, 560.28908995, 528.87662373, ..., 518.74699087,\n",
       "        518.67379822, 500.18344376],\n",
       "       [479.7201979 , 528.87662373, 548.7854355 , ..., 488.42532325,\n",
       "        488.26022602, 471.04514173],\n",
       "       [427.39846149, 464.25677258, 475.06124134, ..., 434.47056589,\n",
       "        434.11619038, 419.07782982]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covariance_matrix = np.cov(X_train_subset.T)\n",
    "covariance_matrix[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([263897.73881923,   5563.57262242,   2553.758691  ,   1931.52841899,\n",
       "         1127.76136041])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalues, evectors = np.linalg.eig(covariance_matrix)\n",
    "evalues[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000004\n",
      "[0.8856646583586443, 0.018671852468060895, 0.008570644935088129, 0.006482383914948712, 0.0037848690346806847, 0.00309791935702875, 0.0023078710843184437, 0.0019813441272988905, 0.0018670796663160856, 0.0013421823776278677, 0.0011909857553348256, 0.0010844205612322287, 0.0010256450734964075, 0.0009346807043260315, 0.0009091055436895487, 0.0008974495897302061, 0.0008389658277320779, 0.000834137882240317, 0.0007749799421074719, 0.0007069390793947006, 0.0006644133185751791, 0.0006263606744890566, 0.000616549119244162, 0.0005878863678340898, 0.0005677438762313714, 0.0005200385892733857, 0.0005178207020070637, 0.0005046660442152526, 0.0004928425866161622, 0.0004768169678091593, 0.0004599695932692947, 0.0004439570082783314, 0.0004374433455004572, 0.00042508303903935653, 0.0004194542339699272, 0.00040721812737647134, 0.00039798631685968505, 0.00038643244308436025, 0.0003838062915468922, 0.00037173515188966945, 0.0003619666262318171, 0.0003645488252589191, 0.00035079409311879417, 0.0003295682833137765, 0.0003268568600537502, 0.0003221485284634922, 0.00032409145296288653, 0.000317431359702351, 0.00030947291903601237, 0.00030543088007958384, 0.00030471314008528053, 0.0002979967184505223, 0.0002956113835523784, 0.0002922662127272196, 0.0002864048461153939, 0.0002800458685094228, 0.00027703322315626586, 0.0002737078390813993, 0.00027156889500970613, 0.00026902194271119983, 0.0002662109835759115, 0.00026439999462259675, 0.00026042809139616277, 0.00025545124993052145, 0.000252012678384607, 0.00025096623258902636, 0.00024755930913940587, 0.00024491974178319, 0.00024136400098606015, 0.00023982877599015517, 0.00023833366141415394, 0.000235611957749564, 0.00023466884913724774, 0.00022594612970934492, 0.00023187174884465122, 0.00023108407797264155, 0.00023005169607682413, 0.0002213251407684966, 0.00021838495050818463, 0.0002161980326793184, 0.0002143984992687773, 0.00021312886463891256, 0.0002123382767556059, 0.00020878085640483693, 0.00020848777044637235, 0.00020643949436626515, 0.00020422489907159902, 0.00020282126842249146, 0.00020144023379777115, 0.0001995585234628702, 0.00019860555774157417, 0.0001962270314961868, 0.00019487874936895423, 0.0001901940304016583, 0.000192856992704751, 0.00017716444920841995, 0.00019250811726118544, 0.0001880417032643616, 0.0001874337506926121, 0.00018520789014661728, 0.00017743787036331128, 0.00017933045644586367, 0.00018388170938133029, 0.0001805180801158609, 0.00018222734800401542, 0.0001820048167805552, 0.00018487797163867516, 0.00017493994808373062, 0.0001761166411881409, 0.00017276121676073234, 0.0001717030723803869, 0.00017025874262272214, 0.0001714093299864083, 0.000168140016108774, 0.00016795733149632973, 0.00016585164037348073, 0.0001645024868886761, 0.00016513606002790205, 0.00016265524296174246, 0.0001640216466171809, 0.0001612665631022021, 0.00016005316259251585, 0.00015877906275027387, 0.0001578095850966796, 0.0001574255076710904, 0.00015639531281605695, 0.0001553563292312455, 0.00015490274227082166, 0.00015251098247887228, 0.00015147126547379215, 0.00015310631417217427, 0.00015102126510653627, 0.0001496955410302039, 0.00014849035347943188, 0.00014751747741197747, 0.00014708109049320638, 0.0001458161903053635, 0.00014563533707329077, 0.00014443205169002645, 0.00014351896179900907, 0.00014241348872026473, 0.00014189061333082332, 0.00014158385256757554, 0.00014074534150992088, 0.0001277469310465485, 0.00013789468016465176, 0.0001390813297793742, 0.0001297285314292467, 0.00012917187684834675, 0.00013096889412551134, 0.00013961809596831683, 0.0001329406794374662, 0.00012828220170676207, 0.00013195801198645406, 0.0001356525529910688, 0.0001346673877272035, 0.00013615074751846545, 0.00012931863375619798, 0.00013887534341273317, 0.000132297012768093, 0.0001352816322661422, 0.00013542081528521721, 0.00012659670063140663, 0.00012604494169536924, 0.00012496895496314693, 0.0001257331692809536, 0.00012559199320074914, 0.0001238173377324647, 0.00012368621297716926, 0.00012289087112932944, 0.00012167111746775718, 0.00012237933661502144, 0.00012208194725329993, 0.00012087369866138999, 0.00010532437892485865, 0.00010627571676540444, 0.00010651393694323806, 0.00011951073126776618, 0.0001188716319613992, 0.000119176565056035, 0.00011975475585275944, 0.00011768828746216352, 0.00010853223155750461, 0.00010751288971497286, 0.00010889529904931166, 0.00011683028731038462, 0.00010955707242487089, 0.00011048874279851239, 0.00011420483994182289, 0.00011516255012498823, 0.00011576144477125721, 0.00011622193383765504, 0.00011645148840972961, 0.00011093455636972047, 0.0001113216547686435, 0.0001123238921486218, 0.00010737080564526576, 0.00011483948622514529, 0.00011168105647893542, 0.00011314620116682441, 0.00011326948685854727, 0.00011284455741924568, 2.096677626589307e-05, 2.207257295261501e-05, 2.335046037233529e-05, 0.00010659109127054669, 0.00010493851092565284, 0.00010476531912814825, 0.00010404812892360983, 0.0001035910843263312, 0.00010321277750094752, 0.00010287149476257717, 0.00010226531909254, 0.00010154981677062853, 0.00010109826934064732, 9.41471645070186e-05, 9.452491117784214e-05, 0.00010141698744867734, 0.00010046046125230139, 0.00010031970569760278, 9.98798447573161e-05, 9.680169857113532e-05, 9.595830462881826e-05, 9.811912856156348e-05, 9.739800518289812e-05, 9.928692263258482e-05, 9.503402693028634e-05, 9.611527354549027e-05, 9.756744643883494e-05, 9.857032414788731e-05, 9.845343645751481e-05, 9.56531859140107e-05, 9.939458076030702e-05, 9.499169890143388e-05, 2.5102823315365978e-05, 2.6732798432154875e-05, 2.8017931857675245e-05, 2.92805109019085e-05, 9.392030377328276e-05, 9.341190163011392e-05, 9.157947908083621e-05, 9.198298040883973e-05, 9.258635752438722e-05, 9.276859155787852e-05, 9.290168897026559e-05, 9.048095683058816e-05, 9.09757029364155e-05, 9.024884991143057e-05, 8.937579598312932e-05, 8.910481488648265e-05, 8.836615642013939e-05, 8.851407915956443e-05, 8.787597463367428e-05, 8.760767506320479e-05, 8.683130952254992e-05, 8.544811441985029e-05, 8.494771624575773e-05, 8.369222292014225e-05, 8.729908390523571e-05, 8.713711938249754e-05, 8.35364563382366e-05, 8.573650403192199e-05, 8.421932437335407e-05, 8.442453978703181e-05, 8.533780744727461e-05, 3.015949188163795e-05, 3.071788247515759e-05, 3.08028283553084e-05, 3.099035944820769e-05, 3.0885155910035594e-05, 3.13357725979209e-05, 3.181308118191597e-05, 3.1568026132768e-05, 8.2320944536539e-05, 8.312082546338116e-05, 8.290086834145389e-05, 8.127719636604703e-05, 8.097685400919959e-05, 8.217087953839423e-05, 8.188754340961286e-05, 8.045645549911402e-05, 7.981352448282112e-05, 7.950966205771819e-05, 7.854297486412724e-05, 7.88910994620972e-05, 7.814469112393413e-05, 7.809265928655063e-05, 7.745389793446226e-05, 7.711908138140394e-05, 7.688938260147995e-05, 7.66232905128669e-05, 7.62963351740455e-05, 7.60552913154814e-05, 7.573528770731859e-05, 7.549361423810353e-05, 6.736975571309234e-05, 6.750161870210306e-05, 6.770361141912122e-05, 6.790601017286938e-05, 7.47471791493383e-05, 6.920443451747915e-05, 7.420839564949054e-05, 7.44737898636356e-05, 7.462261737133368e-05, 7.36209316316828e-05, 6.877039571806215e-05, 6.959299667509094e-05, 7.295319178977566e-05, 6.994643387464804e-05, 7.389992982927842e-05, 7.021783466020787e-05, 6.844633323779413e-05, 6.845592119392749e-05, 7.307132809370545e-05, 7.180886458138132e-05, 7.189498021327462e-05, 7.1509165815099e-05, 7.099667783184593e-05, 7.247353000269732e-05, 7.089407888138903e-05, 3.1848003172725426e-05, 3.1927141311788295e-05, 3.235960861101814e-05, 3.2492276863877724e-05, 3.256635504020771e-05, 6.638949621802998e-05, 6.671745302382081e-05, 3.2661854398802845e-05, 3.2836731585411705e-05, 3.301326643729684e-05, 3.317959988018478e-05, 3.3095175247748944e-05, 3.334126954201114e-05, 6.574833863853743e-05, 6.61104550304713e-05, 6.472583574029099e-05, 6.529301191035995e-05, 6.556251257373767e-05, 6.492770745389563e-05, 3.3550446125409634e-05, 3.364045546619031e-05, 3.38169982241082e-05, 3.4250633055076593e-05, 3.4066285010652756e-05, 3.402235040393348e-05, 3.460933609894622e-05, 3.467363440421078e-05, 6.424853781972067e-05, 6.344408322165333e-05, 6.40388066135187e-05, 6.375057075680802e-05, 6.296087331036136e-05, 3.448684209333167e-05, 6.094761482388605e-05, 6.130234741254023e-05, 6.227660872852102e-05, 6.177076090743344e-05, 6.256960023052334e-05, 6.268845759807076e-05, 3.4946029687264264e-05, 3.519406652071567e-05, 3.5308381856151755e-05, 3.548194805508522e-05, 6.22022198348593e-05, 6.112367729734313e-05, 3.5821302909841824e-05, 3.570029227369462e-05, 3.593742682691408e-05, 6.0232272458156184e-05, 6.000202887679273e-05, 5.981115079676585e-05, 3.605152650951011e-05, 3.751995138484497e-05, 3.625936982730544e-05, 3.6354533356167525e-05, 5.9617036998426534e-05, 3.7273798428902034e-05, 5.939102201315017e-05, 5.898798588219203e-05, 5.919571194623242e-05, 3.65975001688144e-05, 5.86716140438796e-05, 3.663737283206367e-05, 5.9241660050303494e-05, 5.830245890256317e-05, 5.812629036796323e-05, 5.785413431478089e-05, 5.851037972577755e-05, 5.361305753327121e-05, 3.695385762109272e-05, 3.6864955882016645e-05, 3.691213354376458e-05, 5.405100674483179e-05, 5.443630024928841e-05, 5.736198155961781e-05, 5.70186592848664e-05, 5.716702239100822e-05, 5.645317882961274e-05, 5.4742615564207495e-05, 5.534406053149764e-05, 5.579280364740683e-05, 5.684569652310208e-05, 5.658824223052346e-05, 5.514541324554296e-05, 5.608033271568616e-05, 5.4988347742247604e-05, 3.784531153822585e-05, 3.7611406708578467e-05, 5.301779675789584e-05, 5.341011160261288e-05, 5.3745028668148735e-05, 5.420621652130305e-05, 3.8076189513319675e-05, 3.8061268108823934e-05, 3.8535961626496276e-05, 3.8446071979621974e-05, 3.834903892246148e-05, 5.019871340196337e-05, 5.067988498850605e-05, 5.085781891293044e-05, 5.241529210800249e-05, 5.107808320643461e-05, 5.205228780856689e-05, 5.177110185083432e-05, 5.253527933425019e-05, 5.123553450055121e-05, 5.1561832305350545e-05, 5.295536681056165e-05, 5.149338995485318e-05, 5.2599955182510936e-05, 5.212743552081221e-05, 3.870645582275044e-05, 3.878400036916913e-05, 3.912725413139211e-05, 3.894875000028761e-05, 5.0119644580871e-05, 3.9316772447523826e-05, 3.9462638737956396e-05, 4.984792096745628e-05, 3.920612368009015e-05, 4.969263207779593e-05, 4.915406348486147e-05, 4.931952261942513e-05, 4.94377493972335e-05, 3.97029655129808e-05, 4.876079983129873e-05, 4.8679717622056606e-05, 4.0060114758015576e-05, 4.902750712120816e-05, 4.847304579459468e-05, 4.8287782168432386e-05, 4.83684201578102e-05, 3.978159108086922e-05, 3.992852892933301e-05, 4.042497423542806e-05, 4.025500214754661e-05, 4.741730569656951e-05, 4.733924621788152e-05, 4.762901302016121e-05, 4.8025933077204065e-05, 4.800746533724671e-05, 4.6899670224140495e-05, 4.7071997738840864e-05, 4.6750733330518724e-05, 4.696724239186285e-05, 4.649628788397015e-05, 4.61739168584208e-05, 4.0272464209829434e-05, 4.062525487594933e-05, 4.071557931130223e-05, 4.598363860029651e-05, 4.628167489568725e-05, 4.088190537262933e-05, 4.066667555341669e-05, 4.104765909820808e-05, 4.570703026802317e-05, 4.563648141049178e-05, 4.5151631601959286e-05, 4.1277893869494066e-05, 4.1368578657388615e-05, 4.482641485203849e-05, 4.085815725229414e-05, 4.549854981851887e-05, 4.506488857886808e-05, 4.440932983326405e-05, 4.11883945066857e-05, 4.588704971276191e-05, 4.4660175643355155e-05, 4.1892259266850595e-05, 4.461905955158331e-05, 4.1996381447996076e-05, 4.36423743203268e-05, 4.154663526214705e-05, 4.234017863668293e-05, 4.345535279651959e-05, 4.210376504324883e-05, 4.313582246557413e-05, 4.257564687745262e-05, 4.407210911584195e-05, 4.291290218376971e-05, 4.3891829304779034e-05, 4.396564741966154e-05, 4.535402423274973e-05, 4.2782659399917106e-05, 4.337469828126991e-05, 4.26889075176182e-05, 4.223784456250339e-05, 4.1615345848042046e-05, 4.4973885744270496e-05, 4.16281181848124e-05, 4.318965340839942e-05, 4.404730098398145e-05, 4.272821160277162e-05]\n"
     ]
    }
   ],
   "source": [
    "explained_variances = []\n",
    "for i in range(len(evalues)):\n",
    "    explained_variances.append(evalues[i] / np.sum(evalues))\n",
    " \n",
    "print(np.sum(explained_variances))\n",
    "print(explained_variances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the first K dimensions for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = evectors[:K]\n",
    "X_transformed = np.dot(X_train_subset,U.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112680, 100)\n"
     ]
    }
   ],
   "source": [
    "print(X_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_part, y_train, y_part = train_test_split(X_transformed, y_train, test_size=0.2, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_part, y_part, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of clusters \n",
    "K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_centroid(X_train, centroids, distance_type = \"l2\"):\n",
    "\n",
    "    m = X_train.shape[0]\n",
    "    idx = np.zeros((m,))\n",
    "    \n",
    "    if distance_type == \"l2\":\n",
    "        for i in range(m):\n",
    "            min_idx = np.argmin(np.linalg.norm(centroids - X_train[i], axis=1))\n",
    "            idx[i] = min_idx\n",
    "        return idx\n",
    "    elif distance_type == \"inf\":\n",
    "        for i in range(m):\n",
    "            min_idx = np.argmin(np.linalg.norm(centroids - X_train[i], axis=1, ord=\"inf\"))\n",
    "            idx[i] = min_idx\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_centroid(X_train, idx, K):\n",
    "    m, n = X_train.shape\n",
    "    centroids = np.zeros((K, n))\n",
    "\n",
    "    for i in range(K):\n",
    "        curr_len = len(X_train[idx == i])\n",
    "        if curr_len == 0:\n",
    "            centroids[i] = X_train[np.random.choice(m, 1)]\n",
    "        else :\n",
    "            centroids[i] = np.sum(X_train[idx == i], axis=0)/curr_len\n",
    "\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X_train, idx, centroids):\n",
    "    m = X_train.shape[0]\n",
    "    K = centroids.shape[0]\n",
    "    loss = 0\n",
    "    for i  in range(K):\n",
    "        loss += np.sum((X_train[idx == i] - centroids[i])**2)\n",
    "\n",
    "    return loss/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "centroids_hist = []\n",
    "\n",
    "for i in range(10):   ## Using 10 random Initializations corrs. to 10 independent runs of KMeans\n",
    "    c0, c1, c2 = np.random.choice(X_train.shape[0], 3)\n",
    "    centroids = X_train[[c0,c1,c2]]\n",
    "    \n",
    "    for iter in range(20):    ## Iterating over the dataset 20 times\n",
    "        idx = assign_centroid(X_train, centroids)\n",
    "        centroids = update_centroid(X_train, idx, 3)\n",
    "    loss = compute_cost(X_train, idx, centroids)\n",
    "    losses.append(loss)\n",
    "    centroids_hist.append(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.argmin(losses) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid = centroids_hist[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the centroids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('saved_model/kmc+pca/centroid', centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.656732592793281\n"
     ]
    }
   ],
   "source": [
    "# finding out the cluster for music \n",
    "total = 0 \n",
    "correct = 0 \n",
    "\n",
    "for i in range(X_valid.shape[0]):\n",
    "    if(y_valid[i][0] != 1):\n",
    "        continue \n",
    "    else : \n",
    "        total += 1 \n",
    "        x = X_valid[i] \n",
    "        d0 = np.linalg.norm(centroid[0] - x)\n",
    "        d1 = np.linalg.norm(centroid[1] - x)\n",
    "        d2 = np.linalg.norm(centroid[2] - x)\n",
    "        if d0 <= d1 and d0 <= d2 : \n",
    "            correct += 1 \n",
    "        \n",
    "print(correct/total)\n",
    "\n",
    "# We get the best accuracy for music = centroid 0 cluster \n",
    "# for music = centroid 0, accuracy = 0.66\n",
    "# music = centroid 1, accuracy = 0.34\n",
    "# music = centroid 2, accuracy = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6706067769897557\n"
     ]
    }
   ],
   "source": [
    "# finding out the cluster for speech \n",
    "total = 0 \n",
    "correct = 0 \n",
    "\n",
    "for i in range(X_valid.shape[0]):\n",
    "    if(y_valid[i][1] != 1):\n",
    "        continue \n",
    "    else : \n",
    "        total += 1 \n",
    "        x = X_valid[i] \n",
    "        d0 = np.linalg.norm(centroid[0] - x)\n",
    "        d1 = np.linalg.norm(centroid[1] - x)\n",
    "        d2 = np.linalg.norm(centroid[2] - x)\n",
    "        if d1 <= d2 and d1 <= d0 : \n",
    "            correct += 1 \n",
    "        \n",
    "print(correct/total)\n",
    "\n",
    "# We get the best accuracy for speech = centroid 1 cluster \n",
    "# for speech = centroid 0, accuracy = 0\n",
    "# speech = centroid 1, accuracy = 0.67\n",
    "# speech = centroid 2, accuracy = 0.33\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above analysis we get that the music is the centroid 0, speech is the centroid 1 and silence is the centroid 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7784877529286475\n"
     ]
    }
   ],
   "source": [
    "total = 0 \n",
    "correct = 0 \n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    total += 1 \n",
    "    x = X_test[i] \n",
    "    d0 = np.linalg.norm(centroid[0] - x)\n",
    "    d1 = np.linalg.norm(centroid[1] - x)\n",
    "    d2 = np.linalg.norm(centroid[2] - x)\n",
    "    \n",
    "    if y_test[i][0] == 1:\n",
    "        if d0 <= d1 and d0 <= d1 : \n",
    "            correct += 1 \n",
    "        \n",
    "    if y_test[i][1] == 1:\n",
    "        if d1 <= d0 and d1 <= d2 : \n",
    "            correct += 1 \n",
    "\n",
    "    if y_test[i][2] == 1:\n",
    "        if d2 <= d1 and d2 <= d0 : \n",
    "            correct += 1 \n",
    "\n",
    "print(correct/total)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5cc71acb9da8a628b75f40db50cc39e280e3dc99ec6abc765b4a72693f1bb475"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('MLSP': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
