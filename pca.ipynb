{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import librosa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('X_train.npy')\n",
    "y_train = np.load('y_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "X_train_subset = X_train[:10000]\n",
    "y_train_subset = y_train[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making data 0 mean and computing covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.mean(X_train_subset, axis=0)\n",
    "X_train_subset = X_train_subset - mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[549.29860001, 513.36601593, 477.06816083, ..., 530.49830076,\n",
       "        529.61574461, 511.27850877],\n",
       "       [513.36601593, 558.54660462, 524.30431997, ..., 515.27249275,\n",
       "        514.43665514, 496.87785177],\n",
       "       [477.06816083, 524.30431997, 542.38025823, ..., 480.63872586,\n",
       "        479.74065634, 463.05682171],\n",
       "       [427.68536239, 463.7834444 , 471.80014971, ..., 431.62508288,\n",
       "        430.42791702, 415.3173658 ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covariance_matrix = np.cov(X_train_subset.T)\n",
    "covariance_matrix[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([264521.50819269,   5687.66425022,   2534.02481444,   1915.00670257,\n",
       "         1099.15973955])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalues, evectors = np.linalg.eig(covariance_matrix)\n",
    "evalues[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999996\n",
      "[0.8860556828459801, 0.019051710635786714, 0.008488107839125436, 0.006414610982281197, 0.0036818054616359285, 0.003040511851901905, 0.0022811019267993953, 0.0019761736566456987, 0.0018502975357352638, 0.0012935351688686781, 0.001155758001531887, 0.00108516919438287, 0.0010264170984062725, 0.0009293155007250596, 0.0009046694558863024, 0.0008705702092405998, 0.0008520123894388361, 0.0008323747995988331, 0.0007709397655240504, 0.0007182434586022405, 0.000669393580138956, 0.0006274164084264372, 0.0006227567298273284, 0.000584002154767179, 0.0005662075968315397, 0.0005337608825147483, 0.0005198317980555218, 0.0005046133141362033, 0.000495678313945689, 0.0004917904878203296, 0.00046058050890171914, 0.0004476816636790948, 0.0004381087692930121, 0.0004293399342143859, 0.0004205009843536306, 0.00041766750803782176, 0.0004105809062308242, 0.00039784831293328824, 0.00038977485471001697, 0.00037162241503379157, 0.0003702920928287853, 0.0003592994279879058, 0.00034994105697639505, 0.00034229374482503295, 0.00033154694459112255, 0.0003266539306211555, 0.0003205979798712251, 0.00031916270807552676, 0.00031468723672001145, 0.0003118405600079922, 0.0003057641019444957, 0.00030509466602217746, 0.0002975185314170729, 0.00029650827823201074, 0.00029237111878328495, 0.0002824214912377988, 0.0002794933144266033, 0.0002774200060514478, 0.00027522381825310265, 0.00026993444016424446, 0.0002652764398926017, 0.00026481636199183724, 0.0002602528179792425, 0.0002578813531659128, 0.0002545290329912937, 0.0002472850539442129, 0.00025032432984446685, 0.0002516100702526556, 0.0002463133079510341, 0.00024098518318401208, 0.00024274288573786063, 0.0002348504063663582, 0.00023729153654320804, 0.0002365898957656309, 0.00023164892233822832, 0.00022903755730713156, 0.00022719016199837045, 0.00022432724992286669, 0.00022379802274956903, 0.0002217904740647764, 0.0002198177393441474, 0.00021469420485674748, 0.0002161411691756028, 0.00021362148167949457, 0.0002115690294269709, 0.00020983269810026568, 0.0002094861612898222, 0.00020274340789548016, 0.00020431576288392058, 0.0002069494591165877, 0.00020095514541384614, 0.0001997649245768893, 0.00019838240285782, 0.00019477943396677998, 0.0001941425228145633, 0.00019429975982847278, 0.00018584465446162224, 0.00018820546719396734, 0.00018982542831232326, 0.0001918836726015914, 0.0001919190322002913, 0.00018493530889735746, 0.00018368885331416997, 0.00018266351505793536, 0.00018227729976199984, 0.0001807833385475073, 0.00016128451595930356, 0.0001793132724333371, 0.00017643885927414642, 0.00017546879747290333, 0.00017296204573627837, 0.0001801400145936831, 0.00016758871130475938, 0.00017065961324071906, 0.00016959838832271028, 0.00016580523285766288, 0.00017249039674832115, 0.00016538970633195302, 0.000163449134949385, 0.00017034738966030264, 0.00016655192365541962, 0.00016364730763753283, 0.0001593257066990868, 0.00015908395702807126, 0.00015878194637174083, 0.00015764463964656615, 0.00015624188202077065, 0.00015573508911976961, 0.00015469753313904935, 0.00015366864386353677, 0.00015311827228705816, 0.0001516553659990868, 0.0001508966252813147, 0.00014987632253492876, 0.00014928722413294263, 0.00014819676885923766, 0.00014778451192032203, 0.00014698106508441363, 0.00014641659988653792, 0.00014501070683212928, 0.00014429607382089954, 0.0001437753091108495, 0.00014275589048624483, 0.00014247311611759184, 0.00014159885428112966, 0.00014057351138227215, 0.00013979622064417736, 0.00013958713095653646, 0.00013820444363760127, 0.00013764975055222592, 0.00013659791048263668, 0.00013617992131689818, 0.00012159046881900359, 0.00013552967682412204, 0.00012238610681123715, 0.00012329257471038895, 0.00013092372662959804, 0.00013235658255895445, 0.00013322989625354993, 0.00013295512655131953, 0.00013445654738589028, 0.00012358565156758114, 0.0001251917118024416, 0.0001300148384372233, 0.0001282684887576548, 0.00012599891580959376, 0.00012983984023878589, 0.00012695288547592444, 0.00012855551117400992, 0.00012747439502206433, 0.00012626047152078306, 0.00012641696316965925, 0.00012197571407187433, 0.0001210068375142998, 0.00012015135720713754, 0.0001198793454429246, 0.00011879628280953246, 0.00011857188611276908, 0.00011770425508268037, 0.00011723255785643892, 0.00011673039006176704, 0.00011621051145274099, 0.00011495918549901644, 0.00011436867854444673, 0.00011464971233066469, 0.00011399077485362272, 0.00010285603231181161, 0.00011312736030320518, 0.00010331500377234379, 0.00011369303134785591, 0.00010354352944331353, 0.00011237008513633977, 0.00011130163233722781, 0.00010418866876991451, 0.00011085661655386368, 0.00011175833772308056, 0.00011002743850772088, 0.00010946194629192914, 0.00010571799020133811, 0.00010662772257399934, 0.00010620440467005292, 0.00010533442503361662, 0.0001089688704039125, 0.00010811723519071626, 0.00010764098822756343, 0.00010782462181624662, 0.00010400531697394574, 0.00010600671561319266, 0.00010868919448332699, 1.8628172722160185e-05, 2.0206263714031226e-05, 2.1590640434963164e-05, 0.00010205436818459221, 0.00010149807652496767, 0.00010043624364833055, 0.00010017084757214154, 9.96124132532063e-05, 9.902168201966656e-05, 9.84408199367241e-05, 8.988692989269721e-05, 9.018822272855046e-05, 9.818728068490964e-05, 9.567983226756246e-05, 9.620130599641821e-05, 9.724535303435022e-05, 9.705582335057927e-05, 9.796474258588274e-05, 9.517892808954826e-05, 9.200116089736904e-05, 9.394940701856283e-05, 9.088971450023945e-05, 9.124892177739072e-05, 9.347823637473401e-05, 9.272201539222953e-05, 9.47163501283547e-05, 9.244857921170549e-05, 9.691393888932205e-05, 9.447821684502432e-05, 9.315172368345648e-05, 9.080547122692148e-05, 2.2621344799375824e-05, 2.3977597362191303e-05, 2.537109050848849e-05, 2.550134373384685e-05, 2.583230463391352e-05, 2.6165167493890784e-05, 2.6378871633031045e-05, 2.667562627373216e-05, 2.7001208581143356e-05, 2.7159811479383833e-05, 2.746766797598468e-05, 8.895699814150369e-05, 8.926236441126756e-05, 8.969481550835927e-05, 8.782993044388603e-05, 8.846001223583175e-05, 8.839689684530268e-05, 8.727300906469764e-05, 8.650055220959371e-05, 8.68361366233847e-05, 8.625993075007428e-05, 8.584447526603894e-05, 8.556519229004922e-05, 8.491951244733956e-05, 8.444609214758704e-05, 8.413251013948561e-05, 8.288611628139926e-05, 8.375435321297056e-05, 8.029940699805612e-05, 8.250450657602255e-05, 8.112861618922291e-05, 8.131264344649305e-05, 8.226146144915797e-05, 8.182886440960497e-05, 2.762794865576408e-05, 2.7878945449677284e-05, 2.8193494444692152e-05, 2.8482201276068152e-05, 2.849802642774863e-05, 2.893769723218825e-05, 2.9023960954453106e-05, 2.9472244793939668e-05, 2.9186749041706358e-05, 2.9351196559196834e-05, 2.938879072635e-05, 8.220003165588484e-05, 8.085495500646773e-05, 7.981992906638692e-05, 7.912999255264345e-05, 7.868249583366901e-05, 7.89957277841112e-05, 7.840261086761361e-05, 7.804213197172994e-05, 7.75882571111919e-05, 7.549612471289631e-05, 7.663354346034336e-05, 7.602854779072001e-05, 7.728721906995176e-05, 7.701083329783272e-05, 7.525135613263036e-05, 7.437658524752734e-05, 7.466625068240395e-05, 7.384832274424782e-05, 7.344185187738126e-05, 7.311556614531237e-05, 7.268986461472465e-05, 7.270994098623396e-05, 6.640716162185673e-05, 7.217461182920788e-05, 7.182568637369446e-05, 6.685052124775318e-05, 6.703413585866081e-05, 7.12652927143831e-05, 6.729452878889644e-05, 6.791202006986528e-05, 6.835505752138444e-05, 6.920388156501007e-05, 6.814736062606107e-05, 6.785920083001446e-05, 7.104590146132901e-05, 7.049327090689676e-05, 7.004341105749671e-05, 6.939961102441813e-05, 7.0777490066334e-05, 7.021999834367074e-05, 6.972052307807226e-05, 2.9855251487601907e-05, 2.9963092502362603e-05, 3.0108482478492926e-05, 3.024114922602615e-05, 3.044793451305098e-05, 3.052560811750807e-05, 6.621195359467526e-05, 6.586859674640794e-05, 6.521857256365541e-05, 6.550637406339901e-05, 6.562489091923108e-05, 3.096092075233922e-05, 3.09757772925987e-05, 3.111915283929696e-05, 3.1431785726032216e-05, 3.150787602097161e-05, 3.171858770906485e-05, 3.179397208725222e-05, 3.202858940381018e-05, 3.2329783913401486e-05, 6.48142807479413e-05, 6.469306995867819e-05, 6.440980646482932e-05, 3.2083378196481095e-05, 6.379443784742737e-05, 6.347127532014569e-05, 6.31811588565592e-05, 6.254716868399507e-05, 6.163070815298274e-05, 6.0828955050185e-05, 6.104378889669017e-05, 6.116076393263891e-05, 6.212855309725688e-05, 6.249697097869365e-05, 6.221223986507356e-05, 3.24781468005888e-05, 3.2666186825628354e-05, 3.2704416574007566e-05, 3.2902185002685695e-05, 6.057901597088415e-05, 3.314075116724837e-05, 5.9833037460186836e-05, 6.01229147850772e-05, 3.487784562993353e-05, 3.335147327960271e-05, 3.328142233449962e-05, 3.4516359006724325e-05, 3.441217042160079e-05, 3.410891863698666e-05, 6.0032676183732254e-05, 5.907787703930426e-05, 3.381502142795691e-05, 3.4188369021897816e-05, 5.9363063274206405e-05, 5.87385961875042e-05, 5.8807865425496966e-05, 3.35660061055332e-05, 3.370804315386633e-05, 5.8391909195549364e-05, 5.8238451809483334e-05, 5.7641354415055286e-05, 5.7893939488778036e-05, 3.364799799014901e-05, 5.311322401583037e-05, 5.683485457944722e-05, 5.707825172581209e-05, 5.722949583961886e-05, 5.7421915897383995e-05, 5.6203959239255156e-05, 5.643139483751019e-05, 5.355993711055642e-05, 5.379837347939873e-05, 5.556405661197498e-05, 5.570435002586181e-05, 5.527183290518622e-05, 5.507156095252967e-05, 5.4124546597583824e-05, 5.4449237389568736e-05, 5.4617518938149074e-05, 5.457682173052479e-05, 5.500802270570385e-05, 3.5370016480920105e-05, 3.52190072836554e-05, 3.4950083206713576e-05, 3.513452152268736e-05, 3.463628894994174e-05, 5.2438726332211175e-05, 5.261152412238987e-05, 5.299371356482579e-05, 5.3409464278706934e-05, 3.633737057176322e-05, 3.614196194911206e-05, 3.571088515236774e-05, 3.5969450762652784e-05, 5.0783621802908145e-05, 5.106655937319326e-05, 3.57991113483938e-05, 3.592224595057755e-05, 5.184833743339883e-05, 5.198738402204423e-05, 5.2076729152574515e-05, 5.161480839328055e-05, 5.165354476655475e-05, 3.674457414356298e-05, 3.690878416623884e-05, 3.7074763894780464e-05, 3.6822427234998695e-05, 3.727002036900421e-05, 5.058716095797021e-05, 5.014247717228672e-05, 4.907218925799566e-05, 4.9468858780912365e-05, 5.0014067823845525e-05, 4.849868590161317e-05, 4.831889037912974e-05, 4.9795013109739667e-05, 4.9344084169502274e-05, 4.8758675713717926e-05, 4.864487950798987e-05, 5.132496283437357e-05, 5.043341684567164e-05, 4.9821820851944864e-05, 4.791988349538159e-05, 4.7825033111508114e-05, 4.768384678704795e-05, 4.7284826153743645e-05, 4.6917376252557707e-05, 4.724367836617601e-05, 4.66784234866245e-05, 4.6318200752167974e-05, 4.608398575262421e-05, 4.237730224587728e-05, 4.213220640131392e-05, 3.753435882320834e-05, 4.182515116891328e-05, 4.1618194173793764e-05, 3.828375440726871e-05, 3.850073265215121e-05, 4.0014073186120826e-05, 3.9664056952174806e-05, 3.811307356029833e-05, 3.789550104430452e-05, 4.1283531276022345e-05, 4.017879961021445e-05, 3.7718436254159717e-05, 3.9827794171298016e-05, 3.930491057187986e-05, 3.7742605434013164e-05, 4.038620326842257e-05, 3.903780078788416e-05, 3.8738698999255735e-05, 4.1013452379374095e-05, 3.919572533472767e-05, 3.8047417604989296e-05, 4.109150533879416e-05, 4.064420934037347e-05, 3.8723326453505174e-05, 3.8928007948426955e-05, 3.796161752716925e-05, 4.141198511066337e-05, 4.6534747388448634e-05, 4.5874958734633114e-05, 4.574944934782948e-05, 4.4315701064027454e-05, 4.0686485127406654e-05, 4.46114125227058e-05, 4.363515885907815e-05, 4.356245035865472e-05, 4.393826380114566e-05, 4.337992881717814e-05, 4.3043961442947616e-05, 4.27751522066143e-05, 4.391445866860505e-05, 4.54507235083251e-05, 4.4835667315689774e-05, 4.4752025316462476e-05, 4.5349749344428104e-05, 4.516709105683019e-05, 4.6506175312763294e-05, 4.506364258371451e-05, 4.2932464941514334e-05, 4.328070499556367e-05, 4.2648194895119875e-05, 4.200421902501866e-05, 4.075412221554054e-05, 4.253424999964297e-05, 4.538196664473334e-05]\n"
     ]
    }
   ],
   "source": [
    "explained_variances = []\n",
    "for i in range(len(evalues)):\n",
    "    explained_variances.append(evalues[i] / np.sum(evalues))\n",
    " \n",
    "print(np.sum(explained_variances))\n",
    "print(explained_variances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the first K dimensions for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = evectors[:K]\n",
    "X_transformed = np.dot(X_train_subset,U.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(X_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the y values to Integer values rather than vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_final = []\n",
    "for i in range(y_train_subset.shape[0]):\n",
    "    if y_train_subset[i][0] == 1 :\n",
    "        y_train_final.append(0)\n",
    "    elif y_train_subset[i][1] == 1 :\n",
    "        y_train_final.append(1)\n",
    "    else:\n",
    "        y_train_final.append(2)\n",
    "\n",
    "y_train_final = np.array(y_train_final)\n",
    "X_train_final = X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_final.shape, y_train_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = Wx + b\n",
    "# 3x1 = (3,100) * (100,1) + (3,1)\n",
    "W = np.random.rand(3, 100)\n",
    "b = np.random.rand(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x, W, b):\n",
    "    y_pred = np.array([np.zeros([3]) for i in range(x.shape[0])])\n",
    "\n",
    "    for i in range(x.shape[0]): \n",
    "        y_pred[i] = (W.dot(x[i].reshape(-1,1)) + b).reshape(-1)\n",
    "\n",
    "    y_pred_prob = np.array([np.zeros([3]) for i in range(y_pred.shape[0])])\n",
    "\n",
    "    from scipy.special import softmax\n",
    "    \n",
    "    for i in range(y_pred.shape[0]):\n",
    "        y_pred_prob[i] = softmax(y_pred[i])\n",
    "\n",
    "    predictions = np.array([np.argmax(i) for i in y_pred_prob])\n",
    "\n",
    "    return y_pred_prob, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob, predictions = forward_pass(X_train_final, W, b)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, target):\n",
    "    correct = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == target[i]:\n",
    "            correct += 1\n",
    "    accuracy = correct/len(predictions)*100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.32000000000001\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy(predictions, y_train_final) #calculating accuracy for our model\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the training and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_final, y_train_final, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x,y,W,b,lr,epochs):\n",
    "    for i in range(epochs):\n",
    "        y_pred_prob, _ = forward_pass(x, W, b)\n",
    "        y_pred_prob[np.arange(x.shape[0]),y] -= 1\n",
    "\n",
    "        grad_W = y_pred_prob.T.dot(x) \n",
    "        grad_b = np.sum(y_pred_prob, axis = 0).reshape(-1,1)\n",
    "\n",
    "        W -= (lr * grad_W)\n",
    "        b -= (lr * grad_b)\n",
    "\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b = train(X_train_final, y_train_final, W, b, 0.01, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on test dataset - 98.05\n"
     ]
    }
   ],
   "source": [
    "testProbabilities, testPredictions = forward_pass(X_test, W, b)\n",
    "\n",
    "correctPreds = 0\n",
    "for i in range(len(testPredictions)):\n",
    "    if testPredictions[i] == y_test[i]:\n",
    "        correctPreds += 1\n",
    "acc = correctPreds / len(testPredictions) * 100\n",
    "print(\"Model accuracy on test dataset - {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('saved_model/pca/W', W)\n",
    "np.save('saved_model/pca/b',b)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5cc71acb9da8a628b75f40db50cc39e280e3dc99ec6abc765b4a72693f1bb475"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('MLSP': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
